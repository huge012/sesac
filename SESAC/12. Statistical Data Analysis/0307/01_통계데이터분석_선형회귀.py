# -*- coding: utf-8 -*-
"""01. 통계데이터분석 - 선형회귀.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XnJk1OEfZPkhL0vt5SW_OfeTUXtbBWO_

##선형 회귀

    몸무게 = w1(키) + w2(허리둘레) + b
    
    y = w1x1 + w2x2 + b
    y : 종속변수
    x1, x2 : 독립변수
    w1, w2 : 가중치
    b : 편향(bias)

각각의 x값 묶음 -> 하나의 y값

    [ [x1, x2, x3, ... ] , [ ... ], [ ... ] ]
    [ y1, y2, y3 ]

실제 데이터들을 가리키는 가장 적합한 그래프를 찾는 방법

* 단순 선형 회귀

* 다중 선형 회귀

예측치와 데이터 사이의 오차가 가장 적어야 함

## 손실 함수

실제 값과 예측값 사이의 오차를 계산하는 함수

* 평균절대오차 MAE : 오차의 절댓값을 평균냄

* 평균제곱오차 MSE : 실제값과 예측값 사이를 제곱하여 평균냄 (오차를 크게 보기 위함)


## 경사하강법
"""

import matplotlib.pyplot as plt

ex_xs = [ [2], [4], [7], [1], [9], [6] ]  # 독립변수 6개
ex_ys = [ 8, 11, 24, 5, 30, 20 ]   # 종속변수 6개

plt.plot(ex_xs[:], ex_ys, 'o', label="example data")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.title("sample data")
plt.show()

plt.figure(figsize=(5, 5))
plt.plot(ex_xs[:], ex_ys, 'o', label="example data")
plt.plot([0, 30], [0*2+2, 30*2+2], label="y=2x+2")
plt.plot([0, 30], [0*2+2, 30*3+2], label="y=3x+2")
plt.plot([0, 30], [0*2+2, 30*4+2], label="y=4x+2")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.xlim(0, 30)
plt.ylim(0, 30)
plt.title("sample data")
plt.show()

import numpy as np

def mse(yp, y): # yp:예측값, y:실제값
  """
  (예측값-실제값)의 제곱에 대한 평균을 구하는 손실함수
  """
  return np.mean(sum(yp-y)**2)

b = 2  # 편항을 2라고 가정하자
mses = []
wpl = np.arange(-10, 10, 0.5)  # 가중치를 -10 ~ 10까지 0.5씩 증가하면서 확인해보자
for wp in wpl :
  yp = wp * np.array(ex_xs) + b # 예측값
  mses.append(mse(yp, ex_ys)) # mse를 구하여 mses에 보관

mi = np.array(mses).argmin()  # 최솟값이 있는 인덱스
plt.plot(wpl, mses, '.')
plt.plot(wpl[mi], mses[mi], 'or')  # 최솟값
plt.xlabel('w')
plt.ylabel('mse')
plt.show()

"""평미분된 수식에 맞게 경사하강 표현, 이에 맞춰 최솟값을 찾는다

편향(가중치)을 알고 있다는 가정 하에(편향에 대한 경사하강법) 적절한 가중치(편향)을 찾는다

    mse = mean(y-yp)^2
        = mean(y-(wx+b))^2
    
    dmse/dw = 2 * mean(y - (wx+b)) * x
    dmse/db = 2 * mean(y - (wx+b))

"""

w = 3  # 가중치를 3이라고 가정하자
mses2 = []
bpl = np.arange(-10, 10, 0.5)  # 편향을 -10 ~ 10까지 0.5씩 증가하면서 확인해보자
for bp in bpl :
  yp = w*np.array(ex_xs) + bp # 예측값
  mses2.append(mse(yp, ex_ys)) # mse를 구하여 mses에 보관

mi2 = np.array(mses2).argmin()  # 최솟값이 있는 인덱스
plt.plot(wpl, mses2, '.')
plt.plot(bpl[mi2], mses2[mi2], 'or')  # 최솟값
plt.xlabel('b')
plt.ylabel('mse2')
plt.show()

"""가중치 구하기
dmse/dw = 2*mean(y-(wx+b))x

편향 구하기
dmse/db = 2mean(y-(wx+b))
"""

def gradient(y, x, w, b ):
  yp = w*x+b
  error = y - yp
  # 경사가 음수이면 오른쪽으로, 경사가 양수이면 왼쪽으로 이동
  wd = - (2*sum(error*x)/len(x)) 
  bd = - (2*sum(error)/len(x))
  return wd, bd

def gradient_descent(x, y, lr=0.0001, epochs=10000):  # lr 클수록 속도 빠름/작을수록 정확도 올라감, epochs 학습 횟수
  """
  경사하강법 함수
  입력 매개변수 : x, y, lr, epochs
    x : 독립변수
    y : 종속변수
    lr : 경사를 이동시킬 때 사용할 비율(lr*경사 만큼 이동)
    epochs : 학습 횟수
  """
  if isinstance(x, list): # x 가 list일 때
    x = np.array(x).reshape(-1)  # 1차원 numpy 배열로 변화
  wbhl = []  # 학습과정을 기록할 컬렉션

  wp = np.random.uniform(-1, 1)  # 가중치 초기값을 -1 ~ 1 사이의 랜덤 값으로 지정
  bp = np.random.uniform(-1, 1)  # 편향 초기값을 -1 ~ 1 사이의 랜덤 값으로 지정
  amse = 0  # 경사를 조절한 후 mse를 기억할 변수, 0으로 초기화
  for epoch in range(epochs):  # epochs 횟수만큼 학습시킴
    bmse = amse # 이전 mse로 설정
    wd, bd = gradient(y, x, wp, bp)  # 경사를 구함(경사에 - 부호를 취한 값)
    yp = wp*x + bp  # 예측
    amse = mse(yp, y)  # 새로 계산한 mse값
    wp = wp - (wd*lr)  # 가중치 조절
    bp = bp - (bd*lr)  # 편향을 조절
    wbhl.append([wp, bp])  # 히스토리에 가중치와 편향을 보관
    if np.abs(bmse - amse) < 0.0001:  # 이전 mse와 이후 mse의 차이가 정해준 값보다 작다면
      break  # 더 이상 학습하지 말고 반복문 탈출
  return wp, bp, wbhl  # 가중치, 편향, 히스토리 반환

wp, bp, wbhl = gradient_descent(ex_xs, ex_ys)
for epoch, (ewp, ebp) in enumerate(wbhl):
  print(f"epoch: {epoch} , w: {ewp} , b:{ebp}")

xs = np.array(ex_xs)
xs = xs[:, 0]  # ex_xs가 2차원배열이므로
ys = np.array(ex_ys)
ys

min_val = min(min(xs), min(ys))
max_val = max(max(xs), max(ys))
for epoch, wb in enumerate(wbhl):
  plt.figure(figsize=(6, 6))
  sx = min_val
  sy = sx*wb[0] + wb[1]
  ex = max_val
  ey = ex*wb[0] + wb[1]

  plt.plot(xs, ys, '.', label='actural')
  plt.plot([sx, ex], [sy, ey], 'r-', label=f'epoch:{epoch} y={wb[0]}*x+{wb[1]}')
  plt.axvline(x=0, color='black')
  plt.axhline(y=0, color='black')
  plt.xlim(min_val, max_val)
  plt.ylim(min_val, max_val)
  plt.legend()
  plt.show()

min_index = -1  # 최솟값이 있는 인덱스를 -1로 초기화
min_mse = np.inf  # mse 최솟값을 무한대로 초기화
for epoch, (wp, bp) in enumerate(wbhl):
  yp = wp*xs + bp
  mse_val = mse(yp, ys)
  if min_mse > mse_val:  # 새로운 값이 더 작다면
    min_mse = mse_val  # 작은 값으로 설정
    min_index = epoch  # 최솟값이 있는 인덱스를 현재 인덱스로 설정
  plt.plot(wp, mse_val, '.')
plt.plot(wbhl[min_index][0], mse_val, 'ro')
plt.show()

from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()  # 모델 생성
lr_model.fit(ex_xs, ex_ys)  # 학습
print(f"w:{lr_model.coef_} b:{lr_model.intercept_}")

pv = lr_model.predict([ [12],[30],[50] ])  # 예측해주세요
print(pv)

from sklearn import datasets

iris_data = datasets.load_iris()  # 샘플(백합) 데이터 불러오기
iris_data

xs = [ [x] for x in iris_data.data[:, 0] ]  # 컬럼 1에 있는 내용만 불러서 2차원 배열의 독립변수 구조
ys = [y for y in iris_data.data[:, 1]]  # 컬럼 1에 있는 내용만 불러서 1차원 배열의 종속변수 구조

wp, bp, wbhl = gradient_descent(xs, ys)
for epoch, (ewp, ebp) in enumerate(wbhl):
  print(f"epoch:{epoch}  w:{ewp}  b:{ebp}  ")

xs2 = np.array(xs)
xs2 = xs2[:, 0]  # xs가 2차원배열이므로
ys2 = np.array(ys)

min_val = min(min(xs2), min(ys2))
max_val = max(max(xs2), max(ys2))
for epoch, wb in enumerate(wbhl):
  plt.figure(figsize=(6, 6))
  sx = min_val
  sy = sx*wb[0] + wb[1]
  ex = max_val
  ey = ex*wb[0] + wb[1]

  plt.plot(xs2, ys2, '.', label='actural')
  plt.plot([sx, ex], [sy, ey], 'r-', label=f'epoch:{epoch} y={wb[0]}*x+{wb[1]}')
  plt.axvline(x=0, color='black')
  plt.axhline(y=0, color='black')
  plt.xlim(min_val, max_val)
  plt.ylim(min_val, max_val)
  plt.legend()
  plt.show()