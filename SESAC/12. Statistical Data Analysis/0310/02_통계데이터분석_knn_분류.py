# -*- coding: utf-8 -*-
"""02. 통계데이터분석 - KNN 분류.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vOQckBAvlHu1dheQNwvy9tCLk2tgMwrG

회귀 vs 분류

키가 174cm, 허리 둘레 38인치 남성인 사람이 있다. 몸무게는 얼마일까?

- 수직선상에 있는 데이터 추측 : 회귀
- 로지스트 회귀

꽃받침 길이 2.4cm, 너비 3.5cm,
꽃잎 길이 6.5cm, 너비 3.1cm 인 iris는 어떤 품종일까?

- 특정 종류에 속하는지 찾기 : 분류

## KNN 분류에 사용할 데이터 - 봇꽃 데이터
"""

from sklearn.datasets import load_iris
iris_data = load_iris()

data = iris_data['data']
iris_data = load_iris()  # 150개 x (setal-length, setal-width, pepal-length, pepal-width)

target = iris_data['target']
target.shape  # 150개 ([:55] => 0, [50:100] => 1, [100:] => 2)

tname = iris_data['target_names']
tname  # 0:setosa,  1:versicolor,  2:virginica

for i, t in enumerate(target):
  print(f"{i}: {tname[t]}")

def distance(x1, x2) :
  return sum((x1 - x2)**2)**(1/2)

import numpy as np
na1 = np.array([1, 2])
na2 = np.array([5, 5])
print(distance(na1, na2))

def find_k_nearest_neighbor_c(xs, ys, tx, k=5):
  sarr = []
  for i, x in enumerate(xs):
    dis = distance(x, tx)
    # 거리순으로 정렬한 이후에도 몇 번째 요소인지 기억하기 위해 (dis, i)를 보관
    sarr.append((dis, i))
  sarr.sort(key = lambda x:x[0])
  nd = {}  # 어떠한 클래스에 속하는지 분포를 파악해서 보관하기 위한 용도
  for x in sarr[:k]:  # k개의 가까운 이웃
    neighbor = ys[x[1]]  # x[0]은 거리, x[1]은 인덱스이므로 원하는 값은 ys[x[1]]
    if neighbor in nd:
      nd[neighbor] += 1
    else:
      nd[neighbor] = 0
  return max(nd, key=nd.get)  # 가장 많이 나온 클래스를 반환

def find_k_nearest_neighbors_c(xs, ys,t_xs, k=5):
  return np.array([find_k_nearest_neighbor_c(xs, ys, tx,k) for tx in t_xs])

data.shape, target.shape  # 독립 변수, 종속 변수

from sklearn.model_selection import train_test_split
# 학습 데이터와 테스트 데이터로 분리
train_xs, test_xs, train_ys, test_ys = train_test_split(data, target)
train_xs.shape, test_xs.shape, train_ys.shape, test_ys.shape

pred_val = find_k_nearest_neighbors_c(train_xs, train_ys, test_xs)

print(f"예측 결과 : {pred_val}")
print(f"실제 결과 : {test_ys}")

import matplotlib.pyplot as plt

plt.plot(pred_val, 'ro', label='predict')
plt.plot(test_ys, 'b.', label='actual')
plt.xlabel("test index")
plt.ylabel("classes")
plt.legend()
plt.show()

def evaluate(actual_ys, predict_ys):
  correct_cnt = 0
  for i, y in enumerate(actual_ys):
    if predict_ys[i] == y:
      correct_cnt += 1
  return correct_cnt/len(actual_ys)

print(evaluate(test_ys, pred_val))

"""ML 개체 사용 (사이킷 런의 KNN 분류 모델)"""

from sklearn.neighbors import KNeighborsClassifier
knc_model = KNeighborsClassifier()  # 모델 객체 생성
knc_model.fit(train_xs, train_ys)  # 학습
pred_val2 = knc_model.predict(test_xs)  # 예측하시오

print(f"예측 결과1 : {pred_val}")
print(f"예측 결과2 : {pred_val2}")
print(f"실제 결과 : {test_ys}")

plt.plot(pred_val, 'ro', label='predict')
plt.plot(pred_val2, 'g*', label='predict2')
plt.plot(test_ys, 'b.', label='actual')
plt.xlabel("test index")
plt.ylabel("classes")
plt.legend()
plt.show()

print(evaluate(test_ys, pred_val))
print(evaluate(test_ys, pred_val2))

"""## 이항 분류(이진 분류)

둘 중 하나를 결정하는 문제

T or F 를 결정하는 문제

1. sigmoid(선형 회귀 알고리즘) -> 로지스틱 회귀(경사하강법)

    0 <= sigmoid(x) <= 1

    0.5 <= sigmoid(x) -> True
    sigmoid(x) < 0.5 -> False

2. KNN 회귀

## 다항 분류



1. KNN 분류
2. 로지스틱 회귀 응용


    클래스가 n개 있을 때, 로지스틱 회귀를 n번 수행

    클래스 1일 확률
    클래스 2일 확률
    클래스 3일 확률
    .
    .
    클래스 n일 확률
    모든 확률의 합 sp = p1 + p2 + ... + pn = 1 이어야 함
    p1 = p1/sp,  p2 = p2/sp,  p3 = p3/sp  로 조정
    (p1 + p2 + p3 + ... + pn)/sp = 1
    
    softmax 방식

ML
- ML
- DL

ML
- 지도 L : 종속 변수와 독립 변수를 이용해서 학습
  - 회귀
  - 분류
- 비지도 L : 데이터를 N개의 집단으로 나누기
  - 강화 : 좋은 데이터에 칭찬 점수 부여
  - 군집 : 비슷한 데이터끼리 모음
  - 이상점 : 데이터를 전부 넣었을 때 오류 검출을 위해 사용, 특징 다른 데이터를 알려줌
"""