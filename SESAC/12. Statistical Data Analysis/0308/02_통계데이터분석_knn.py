# -*- coding: utf-8 -*-
"""02. 통계데이터분석 - KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfB9hne6uXHxTMbrFNNJiGg96aKYN5Dv

# KNN

k개의 가장 인접한 값의 평균

1. k개의 이웃 찾기
"""

from sklearn.datasets import load_iris  # 백합 정보

iris_data = load_iris()  # iris 데이터 확인
iris_data  # 독립변수의 각 속성/특성 : attribute, 총 4개의 attribute로 이루어진 독립변수

data = iris_data['data']  # key가 data인 독립변수
target = iris_data['target']  # 종속변수
tnames = iris_data['target_names']  # 종속변수에 대한 레이블
for i,y in enumerate(target):
  print(f'{i}:{tnames[y]}')

sepal_lengths = data[:50,[0]] #setosa 품종의 sepal_length만 추출(독립변수로 사용하기 위해 2차원 배열)
sepal_widths = data[:50,1] #setosa 품종의 sepal_width만 추출(종속변수로 사용하기 위해 1차원 배열)

"""꽃받침의 폭이 얼마나 되는지 K-NN 회귀로 알아보자"""

import matplotlib.pyplot as plt
plt.plot(sepal_lengths[:,0],sepal_widths,'ro',label='setosa')
plt.xlabel('sepal-length')
plt.ylabel('sepal-width')
plt.yticks([0,max(sepal_widths)])
plt.title('iris - setosa')
plt.show()

# 일부분을 학습시키고 나머지로 학습내용을 평가하자
train_xs = sepal_lengths[:len(sepal_lengths)*2//3]  # 0 ~ 2/3
train_ys = sepal_widths[:len(sepal_widths)*2//3]
test_xs = sepal_lengths[len(sepal_lengths)*2//3:]  # 2/3 ~ 끝
test_ys = sepal_widths[len(sepal_widths)*2//3:]
print(f"학습용 길이:{len(train_ys)} 테스트용 길이:{len(test_ys)}")

"""거리 계산"""

import numpy as np

def distance(x1, x2):
  if isinstance(x1, int) and isinstance(x2, int):  # 두 개의 값이 모두 int 형식일 때 (scalar값일 때)
    return np.abs(x2-x1)  # 차이의 절대값을 반환
  if isinstance(x1, list) and isinstance(x2, list):
    x1 = np.array(x1)
    x2 = np.array(x2)
  return sum((x1-x2)**2)**(1/2)

a = [1,2]
b = [3,4]
# re = sum((a-b)**2)**(1/2)   # list끼리 - 연산 불가
a = np.array([1,2])
b = np.array([3,4])
re = sum((a-b)**2)**(1/2)  # np.array는 - 연산 가능
re

na1 = np.array([1, 2])
na2 = np.array([4, 6])
assert distance(na1, na2) == 5.0, "거리 계산 오류"  # assert 테스트, 지정된 값이 안 나오면 오류 처리 됨

"""k개의 가까운 이웃을 찾아라"""

def find_k_nearest_neighbor(xs, ys, tx, k=5):
  """
  입력 매개변수 : xs, ys, tx, k
  xs : 독립변수 (학습 데이터)
  ys : 종속변수 (학습 데이터)
  tx : 독립변수 (예측에 사용할 신입)
  k : 찾을 이웃 수
  반환 : k개의 이웃의 y 평균값
  """
  sarr=[]
  for i, x in enumerate(xs):
    dis = distance(x, tx)  # 거리 구하기
    sarr.append([dis, i])  # 계산한 거리와 인덱스를 보관

  sarr.sort(key=lambda x:x[0])  # dis 순으로 정렬

  k = min(k, len(sarr))  # 현재 학습 데이터 개수와 k중 최솟값을 k로 확정
  neighbors = [ x[1] for x in sarr[:k] ]  # 가장 가까운 이웃 k개의 인덱스로 리스트 구성
  return sum(ys[neighbors])/k  # 이웃의 평균 값을 반환

# 여러개의 테스트 돌려보기
def find_k_nearest_neighbors(xs, ys, t_xs, k=5):
  return [find_k_nearest_neighbor(xs, ys, tx, k) for tx in t_xs]

pred_val = find_k_nearest_neighbors(train_xs, train_ys, test_xs)
pred_val

plt.plot(pred_val, 'ro', label='predict')  # 예측값
plt.plot(test_ys, 'b.', label='actual')  # 실제 수치
plt.ylim(0, 5)  # 리밋 설정
plt.legend()
plt.xlabel("index")
plt.ylabel("sepal-width")
plt.title("iris - setosa")
plt.show()

# 수학적 접근
print(np.mean(np.abs(pred_val - test_ys)/test_ys))

from sklearn.neighbors import KNeighborsRegressor

knr_model = KNeighborsRegressor()  # 모델 객체 생성, n_neighbor 디폴트5
knr_model.fit(train_xs, train_ys)  # 학습하세요
pred_val2 = knr_model.predict(test_xs)  # 예측하세요

plt.plot(pred_val, 'r.', label='use our predict')  # 직접 짠 코드 예측값
plt.plot(pred_val2, 'b.', label='KNeighborsRegressor')  # KNeighborsRegressor 예측값
plt.plot(test_ys, 'go', label='actual')  # 실제 수치
plt.ylim(0, 5)  # 리밋 설정
plt.legend()
plt.xlabel("index")
plt.ylabel("sepal-width")
plt.title("iris - setosa")
plt.show()

print(np.mean(np.abs(pred_val2-test_ys)/test_ys))

# 우리가 사용한 표본 범위 이상을 요청해도 기존 학습 데이터의 평균에서 예측함
print(find_k_nearest_neighbor(train_xs, train_ys, [3.4]))
print(find_k_nearest_neighbor(train_xs, train_ys, [6]))
print(find_k_nearest_neighbor(train_xs, train_ys, [8]))
print(find_k_nearest_neighbor(train_xs, train_ys, [80]))

"""KNN에서 독립변수에 특성이 여러 개이고 특성에 따라 크기의 차이가 상당할 때"""

# 독립변수 : 키(cm), 몸무게(kg/100)
train_xs2 = np.array([ [180, 0.84], [190, 0.89], [120, 0.32], [150, 0.49], [160, 0.58], [170, 0.65] ])
# 종속변수 : 허리둘레
train_ys2 = np.array([ 32, 33, 24, 26, 30, 31 ])

# [210, 1.10] - 가장 가까운 데이터 [190, 0.89]의 결과 [33]을 따라감
print(find_k_nearest_neighbor(train_xs2, train_ys2, [210, 1.10], k=1)) 
# [188, 0.83] - 가까운 이웃 선택이 어려운 케이스
print(find_k_nearest_neighbor(train_xs2, train_ys2, [186, 0.84], k=1))

distance([190, 0.89], [186, 0.84]), distance([180, 0.84], [186, 0.84])

plt.plot(train_xs2[:,0],train_xs2[:, 1],'ro',label='sample')
plt.plot([186], [0.84], 'b^', label="test")
plt.xlim(0, 210)
plt.ylim(0, 1.10)
plt.legend()
plt.show()

# 값 크기 차이가 커서 데이터 표기가 어려움
plt.figure(figsize=(5,5))
plt.plot(train_xs2[:,0],train_xs2[:, 1],'ro',label='sample')
plt.plot([186], [0.84], 'b^', label="test")
plt.xlim(0, 210)
plt.ylim(0, 210)
plt.legend()
plt.show()

heights = train_xs2[:, 0]
hm = heights.mean()
hs = heights.std()
hm, hs  # 평균, 표준편차
heights2 = heights - hm  # 수평 이동
heights3 = heights2 / hs  # 표준편차만큼 비율 조절

hm3 = heights3.mean()
hs3 = heights3.std()
hm3, hs3  # 평균 0, 표준편차 1인 표준정규분포 만들어줌 - 스케일조절

weights = train_xs2[:, 1]
wm = weights.mean()
ws = weights.std()
wm, ws # 평균, 표준편차

weights2 = weights - wm  # 수평 이동
weights3 = weights2 / ws  # 표준편차만큼 비율 조절

wm3 = weights3.mean()
ws3 =heights3.std()
wm3, ws3  # 평균 0, 표준편차 1인 표준정규분포 만들어줌 - 스케일 조절

rescaled_train_xs = np.array([ [heights3[i], w] for i, w in enumerate(weights3)])
rescaled_train_xs

h = 186
w = 0.84
w3 = (w-wm)/ws
h3 = (h-hm)/hs
h3, w3

print(find_k_nearest_neighbor(rescaled_train_xs, train_ys2, [h3, w3], k=1))

plt.figure(figsize=(5,5))
plt.plot(rescaled_train_xs[:,0],rescaled_train_xs[:, 1],'ro',label='sample')
plt.plot([h3], [w3], 'b^', label="test")
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.legend()
plt.show()

"""선형 회귀 VS KNN 회귀

회귀 모델 VS 분류 모델(KNN 분류) VS 군집 모델 (KMeans)
"""