# -*- coding: utf-8 -*-
"""d14_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YKl9SCY1Rmndbhg4zMTXp7X0DyifGRmP
"""

# Commented out IPython magic to ensure Python compatibility.
# %run my_init.py

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train_scaled = (X_train / 255.0)
X_train_scaled.shape

from sklearn.model_selection import train_test_split

X_train_scaled, X_val_scaled, y_train, y_val = \
  train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)
X_train_scaled.shape, X_val_scaled.shape, y_train.shape, y_val.shape

def model_fn2(*layers):
  model = keras.Sequential(name="fashion_mnist_model")
  model.add(keras.layers.Flatten(input_shape=(28, 28), name="input"))
  model.add(keras.layers.Dense(100, activation='relu', name="hidden_1"))
  for layer in layers:
    model.add(layer)
  model.add(keras.layers.Dense(10, activation='softmax', name="output"))
  return model

model = model_fn2()

model.summary()

model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, epochs=20, verbose=0, validation_data=(X_val_scaled, y_val))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()

model = model_fn2(keras.layers.Dropout(0.3))
model.summary()

model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, epochs=20, verbose=0, validation_data=(X_val_scaled, y_val))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()

models, histories = [], []
rates = np.arange(0.0, 0.6, 0.1)
for rate in rates:  # range() : 인트형만 반환, np.arange() : float형도 반환
  model = model_fn2(keras.layers.Dropout(rate))
  model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
  history = model.fit(X_train_scaled, y_train, epochs=20, verbose=0, validation_data=(X_val_scaled, y_val))
  models.append(model)
  histories.append(history)

fig, axes = plt.subplots(rates.size, 1, figsize=(6, 20))
for i, ax in enumerate(axes):
  ax.plot(histories[i].history['loss'])
  ax.plot(histories[i].history['val_loss'])
  ax.set_title(f"dropout rate : {rates[i]:.1f}")
  ax.set_xlabel('epoch')
  ax.set_ylabel('loss')
  ax.legend(['train','val'])
plt.tight_layout()
plt.show()

models

model = models[4]  # dropout 0.3인 데이터

model.layers # 해당 모델의 구조

model.layers[2] # dropout

?keras.layers.Dropout

model.summary()

model.save_weights('model-drop-0.3-weights.h5')

# dropout을 뺀 model - 구조가 달라짐, 파라미터 개수 다름
model = model_fn2()
model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, epochs=20, verbose=0, validation_data=(X_val_scaled, y_val))

model.load_weights('model-drop-0.3-weights.h5')

model.summary() # dropout 미포함, Trainable params는 같으므로 로딩 가능

model.predict(X_val_scaled)

model = model_fn2(keras.layers.Dense(10, activation="relu"))
model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, epochs=20, verbose=0, validation_data=(X_val_scaled, y_val))

model.summary()

model.load_weights('model-drop-0.3-weights.h5') # Trainable params의 개수가 저장 데이터와 맞지 않아서 읽어올 수 없음
model.summary()

model.save('model-hidden_2.h5')

# model.load()는 말이 안 됨, 모델에서 모델을 불러올 수 없음, keras.models.load_model() 사용
new_model = keras.models.load_model('model-hidden_2.h5')
new_model.summary() # 훈련된 모델

X_test_scaled = (X_test / 255.0)
X_test_scaled

new_model.predict(X_val_scaled) # 바로 예측 가능, 이미 훈련된 모델을 불러왔기 때문

X_test_scaled.shape

new_model.predict(X_test_scaled[0:1])  # 모든 데이터가 0, 맨 마지막 클래스

y_test_pred_proba = new_model.predict(X_test_scaled)
y_test_pred_proba.shape

np.argmax(y_test_pred_proba, axis=1).shape # argmax에는 가장 큰 값의 인덱스가 들어가 있음(확률이 가장 큰 경우의 클래스)

y_test_pred = np.argmax(y_test_pred_proba, axis=1)
np.sum(y_test_pred == y_test)  # 총 몇 개를 맞췄는지

np.mean(y_test_pred == y_test)  # 정확도

model.evaluate(X_test_scaled, y_test)  # 정확도, 내부에서 predict 한 번 더 진행하게 됨