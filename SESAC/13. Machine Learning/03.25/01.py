# -*- coding: utf-8 -*-
"""01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CnV3OXkv2KcMr9V4ksq8OIPcxFOEsT_p
"""

# Commented out IPython magic to ensure Python compatibility.
# %run my_init.py

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

wine = pd.read_csv("https://bit.ly/wine_csv_data")
wine.head(3)

data = wine.drop('class', axis=1).to_numpy()
data.shape

target = wine['class'].to_numpy()
target.shape

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

train_input.shape, train_target.shape

test_input.shape, test_target.shape

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

from sklearn.model_selection import cross_validate

scores = cross_validate(dt, train_input, train_target, cv=3, return_train_score=True)
scores

np.mean(scores['test_score']) # validation score

np.mean(scores['train_score'])

from sklearn.model_selection import KFold, StratifiedKFold

kfold = KFold(3, shuffle=True, random_state=42)
skfold = StratifiedKFold(3, shuffle=True, random_state=42)

scores = cross_validate(dt, train_input, train_target, cv=skfold, return_train_score=True)
scores

np.arange(0.0001, 0.0006, 0.0001)

for i, v in enumerate(np.arange(0.0003, 0.0008, 0.0001)):
  dt = DecisionTreeClassifier(min_impurity_decrease=v, random_state=42)
  scores = cross_validate(dt, train_input, train_target, cv=skfold, return_train_score=True)
  print(f"[{i+1}]")
  print(f"train score: {np.mean(scores['train_score'])}")
  print(f"test score: {np.mean(scores['test_score'])}", end="\n\n")

from sklearn.model_selection import GridSearchCV

dt = DecisionTreeClassifier(random_state=42)
params = {
    "max_depth": range(3, 6),
    "min_samples_split": [2, 5, 8],
    "min_impurity_decrease": np.arange(0.0001, 0.0006, 0.0001)
}

3 * 3 * 5 * 5

gs = GridSearchCV(dt, params, cv=5, n_jobs=-1)
gs.fit(train_input, train_target)

gs.best_estimator_

gs.best_params_

gs.best_score_

gs.cv_results_.keys()

gs.cv_results_['mean_test_score']

np.max(gs.cv_results_['mean_test_score'])

best_model = gs.best_estimator_
best_model.get_params()

best_model.score(train_input, train_target)

best_model.score(test_input, test_target)

"""### probability distribution"""

from scipy.stats import uniform, randint, norm, poisson, gamma, beta, bernoulli  # 확률 분포를 지원하는 모듈

ud = uniform(0, 1)
ud.rvs(10, random_state=42)  # random variates

ud.rvs((4, 5), random_state=42)

nd = norm(60, 5) # 평균 60, 표준편차 5 설정
nd.rvs((4, 5), random_state=42)

params = {
    "max_depth": randint(3, 50),
    "min_samples_split": randint(2, 20),
    "min_impurity_decrease": uniform(0.0001, 0.001)
}

from sklearn.model_selection import RandomizedSearchCV

# 객체 생성(결정트리, ) - 분포 전달해야 함
rs = RandomizedSearchCV(dt, params, n_iter=500, n_jobs=-1, cv=10, random_state=42)
rs.fit(train_input, train_target)

rs.best_score_  # cv 적용해서 나온 점수

rs.best_params_

dt = rs.best_estimator_
accuracy_score(train_target, dt.predict(train_input))  # 학습 성능

accuracy_score(test_target, dt.predict(test_input))  # test 데이터로 첫 검증, bestfit